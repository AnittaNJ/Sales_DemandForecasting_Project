{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnittaNJ/Sales_DemandForecasting_Project/blob/main/Sales_DemandForecasting_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j5Bse6izKKh"
      },
      "source": [
        "## **Business Context**\n",
        "\n",
        "The Nielsen BookScan service is the world’s largest continuous book sales tracking service in the world, operating in the UK, Ireland, Australia, New Zealand, India, South Africa, Italy, Spain, Mexico, Brazil, Poland, and Colombia. Nielsen BookScan collects transactional data at the point of sale, directly from tills and dispatch systems of all major book retailers. This ensures detailed and highly accurate sales information on which books are selling and at what price, giving clients the most up-to-date and relevant data. The Nielsen BookScan Total Consumer Market (TCM) data covers approximately 90% of all retail print book purchases in the UK. The remaining sites are specialised, such as gift shops, specialist booksellers, and tourist information centres.\n",
        "\n",
        "\n",
        "Nielsen BookScan can be used to:\n",
        "Monitor titles and authors against the competition and overall market.\n",
        "Analyse pricing and discounting by format or category.\n",
        "Gauge the success of marketing campaigns and promotions.\n",
        "See which categories are growing and declining.\n",
        "Learn what works in your market and how that might differ from other countries.\n",
        "\n",
        "Nielsen BookScan sales data can be analysed by various criteria, including category, publisher, and format,\n",
        "allowing users to see which genres are selling in which format. Users can track market trends to see which titles are driving the results, and patterns can easily be interpreted. In addition, the actual selling price is included. This inclusion makes it easier to identify trends for the level of discounting (e.g. by title, author, genre, format, region, and publisher) when analysing book sales.\n",
        "\n",
        "\n",
        "**Project context**\n",
        "\n",
        "Nielsen is seeking to invest in developing a new service aimed at small to medium-sized independent publishers. This service is aimed at supporting publishers in using historical sales data to make data-driven decisions about their future investment in new publications. Their publisher customers are interested in being able to make more accurate predictions of the overall sales profile post-publication for better stock control and initial investment, but they are also interested in understanding the useful economic life span that a title may have.\n",
        "\n",
        "Nielsen is targeting small to medium-sized independent publishers as their research has shown that there is a strong demand for this insight, but businesses cannot invest in this infrastructure and would pay a premium to have access to quality-assured data and analysis in this area. Producing a new publication requires a significant upfront investment, and they would like to be able to more accurately identify books with strong long-term potential. More specifically, they are looking for titles with sales patterns that exhibit well-established seasonal patterns and positive trends that show potential great returns and to learn more about these types of publications. Nielsen will then apply this understanding to their commission and print volume strategy to be more successful in acquiring titles that have longevity. Additionally, this will enable them to deliver better returns by ensuring the correct stock levels in relation to demand and avoiding over or understocking, which can be costly.\n",
        "\n",
        "You will notice that some titles experience fluctuations in sales due to various factors, such as increased media attention or cultural relevance (e.g. the recent resurgence of interest in George Orwell’s 1984). However, certain books endure over time and are often studied in academic settings for their deeper significance.\n",
        "\n",
        "For this project, Nielsen has provided you with two data sets. The objective is to identify sales patterns that demonstrate seasonal trends or any other traits, providing insights to inform reordering, restocking, and reprinting decisions for various books (by their International Standard Book Number, or ISBN).\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Objective**\n",
        "\n",
        "Conduct a comprehensive analysis on selected books from Nielsen's data, identifying key sales patterns that exhibit clear seasonal trends or other distinctive characteristics. These insights will serve as a data-driven foundation for optimising procurement, re-ordering, and stocking decisions, ensuring efficient inventory management."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DlV80XY1eAh"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_M_ocIS790mw"
      },
      "outputs": [],
      "source": [
        "!pip install pmdarima\n",
        "!pip install -U keras-tuner\n",
        "!pip install scikit-learn\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0P8kYcRyqsF"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gdown\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from pmdarima import auto_arima\n",
        "import pmdarima as pm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error, make_scorer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras_tuner import RandomSearch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVX-fhdbMWfo"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mCvFEAQ1weE"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g03k6TfjxucV"
      },
      "outputs": [],
      "source": [
        "destination = 'ISBN_List.xlsx'\n",
        "\n",
        "# Construct the download URL\n",
        "download_url = f''\n",
        "\n",
        "# Download the file using gdown\n",
        "gdown.download(download_url, destination, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqECXWkEym-6"
      },
      "outputs": [],
      "source": [
        "destination = 'Weekly_trend.xlsx'\n",
        "\n",
        "# Construct the download URL\n",
        "download_url = f''\n",
        "\n",
        "# Download the file using gdown\n",
        "gdown.download(download_url, destination, quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbb-F_lwdA9c"
      },
      "source": [
        "### ISBN_List File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAsoQ9p9w_XC"
      },
      "outputs": [],
      "source": [
        "# Importing the data file ISBN_List.xlsx into a dataframe.\n",
        "dfs = pd.read_excel('ISBN_List.xlsx', sheet_name=None)\n",
        "\n",
        "# Concatenate all sheets into a single dataframe\n",
        "isbn_df = pd.concat(dfs.values(), ignore_index=True)\n",
        "\n",
        "# Display the combined dataframe\n",
        "isbn_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZr78Jddx50j"
      },
      "outputs": [],
      "source": [
        "isbn_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEpQNi63Tpsi"
      },
      "outputs": [],
      "source": [
        "isbn_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owqdUrFsdof1"
      },
      "source": [
        "### Weekly_Trend File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNjwFks48K2d"
      },
      "outputs": [],
      "source": [
        "# Importing the data file Weekly_trend.xlsx into a dataframe.\n",
        "dfs = pd.read_excel('Weekly_trend.xlsx', sheet_name=None)\n",
        "\n",
        "# Concatenate all sheets into a single dataframe\n",
        "weekly_df = pd.concat(dfs.values(), ignore_index=True)\n",
        "\n",
        "# Display the combined dataframe\n",
        "weekly_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGUfdQfM9a2u"
      },
      "outputs": [],
      "source": [
        "weekly_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBtNANXN8eYK"
      },
      "outputs": [],
      "source": [
        "weekly_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n_RkxG0hliO"
      },
      "source": [
        "## Data Investigation and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slA2H2tpjZM7"
      },
      "outputs": [],
      "source": [
        "# Resample to weekly frequency ending on saturday, mean of the 'Volume' column\n",
        "weekly_df = weekly_df.set_index('End Date')\n",
        "weekly_sat_df = (\n",
        "    weekly_df\n",
        "    .groupby(['ISBN', 'Title'])  # Group by ISBN and Title\n",
        "    .resample('W-SAT')           # Resample to weekly frequency ending on Saturday\n",
        "    .agg({\n",
        "        'Volume': 'sum'           # Sum the 'Volume' column to get total sales per week\n",
        "    })\n",
        "    .fillna(0)\n",
        "    .reset_index()                # Reset index to keep 'ISBN' and 'Title' as columns\n",
        ")\n",
        "\n",
        "# Display the final DataFrame\n",
        "weekly_sat_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jTCF4vt_xk2"
      },
      "outputs": [],
      "source": [
        "# info about dataframe\n",
        "weekly_sat_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8dSdf8ztBs6"
      },
      "outputs": [],
      "source": [
        "# Convert the ISBNs to a string type\n",
        "weekly_sat_df['ISBN'] = weekly_sat_df['ISBN'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn-bH5zPtIhL"
      },
      "outputs": [],
      "source": [
        "# Convert the 'End Date' column to datetime object type\n",
        "weekly_sat_df['End Date'] = pd.to_datetime(weekly_sat_df['End Date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXcavpUOBCD6"
      },
      "outputs": [],
      "source": [
        "# Filtering out the ISBNs wherein sales data exists beyond 2024-07-01.\n",
        "isbns_beyond = weekly_sat_df[weekly_sat_df['End Date'] > '2024-07-01']['ISBN'].unique()\n",
        "for i, isbn in enumerate(isbns_beyond):\n",
        "    print(f\"{i+1}. {isbn}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFfP5zTQuh1q"
      },
      "outputs": [],
      "source": [
        "# Plot the data of each ISBNs from the previous step using loop\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for isbn in isbns_beyond:\n",
        "  df_temp = weekly_sat_df[weekly_sat_df['ISBN'] == isbn]\n",
        "  book_title = df_temp['Title'].iloc[0]\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  plt.plot(df_temp['End Date'], df_temp['Volume'])\n",
        "  plt.title(f\"Sales data for ISBN {isbn} - {book_title}\")\n",
        "  plt.xlabel('Date')\n",
        "  plt.ylabel('Volume')\n",
        "  plt.grid(True)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2zjWfTJWyB4"
      },
      "source": [
        "The Alchemist and The Very Hungry Caterpillar were selected to perform further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thspIgdwlFHi"
      },
      "outputs": [],
      "source": [
        "# creating a dataframe to include The Alchemist and The Very Hungry Caterpillar books.\n",
        "books_of_interest = ['Alchemist, The', 'Very Hungry Caterpillar, The']\n",
        "df_books_of_interest= weekly_sat_df[weekly_sat_df['Title'].isin(books_of_interest)]\n",
        "df_books_of_interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCiphoG3lT89"
      },
      "outputs": [],
      "source": [
        "# Filter the sales data for both these books to retain the date range >2012-01-01, until the final datapoint.\n",
        "df_books_of_interest = df_books_of_interest[df_books_of_interest['End Date'] > '2012-01-01']\n",
        "df_books_of_interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vtkAA1f7k7C"
      },
      "outputs": [],
      "source": [
        "# information about df_books_of_interest dataframe\n",
        "df_books_of_interest.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPSgzK0j4TAq"
      },
      "outputs": [],
      "source": [
        "# setting the 'End Date' as the index\n",
        "df_books_of_interest = df_books_of_interest.set_index(['End Date'])\n",
        "df_books_of_interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwWraMsoFdpK"
      },
      "outputs": [],
      "source": [
        "# created a separate dataframe for each book.\n",
        "alchemist = df_books_of_interest[df_books_of_interest['Title'] == 'Alchemist, The']\n",
        "caterpillar = df_books_of_interest[df_books_of_interest['Title'] == 'Very Hungry Caterpillar, The']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8CttHkkFXZH"
      },
      "outputs": [],
      "source": [
        "# checking the sales pattern of each book.\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the time series for The Alchemist\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(alchemist.index, alchemist['Volume'], label='Sales', color='blue')\n",
        "plt.title(\"Sales Data for 'The Alchemist'\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Sales\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# Plotting the time series for The Very Hungry Caterpillar\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(caterpillar.index, caterpillar['Volume'], label='Sales', color='green')\n",
        "plt.title(\"Sales Data for 'The Very Hungry Caterpillar'\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Sales\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfY76NuhWuhi"
      },
      "source": [
        "## Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwNXFwRgB99d"
      },
      "outputs": [],
      "source": [
        "for book_title in books_of_interest:\n",
        "  df_book = df_books_of_interest[df_books_of_interest['Title'] == book_title]\n",
        "  sales = df_book['Volume']\n",
        "  # Check if the DataFrame is not empty and has enough data points\n",
        "  if not df_book.empty and len(sales) >= 24:\n",
        "      print(f\"\\nDecomposition for: {book_title}\")\n",
        "\n",
        "      # Determine whether to use additive or multiplicative decomposition\n",
        "      # First, attempt additive decomposition\n",
        "      try:\n",
        "          decomposition = seasonal_decompose(sales, model='additive', period=52)\n",
        "\n",
        "          # Plot the decomposition components\n",
        "          plt.figure(figsize=(12, 8))\n",
        "          plt.subplot(411)\n",
        "          plt.plot(df_book.index, sales, label='Original', color='blue')\n",
        "          plt.title(f'Original Sales Data for {book_title}')\n",
        "          plt.legend(loc='best')\n",
        "\n",
        "          plt.subplot(412)\n",
        "          plt.plot(decomposition.trend, label='Trend', color='orange')\n",
        "          plt.title('Trend Component')\n",
        "          plt.legend(loc='best')\n",
        "\n",
        "          plt.subplot(413)\n",
        "          plt.plot(decomposition.seasonal, label='Seasonality', color='green')\n",
        "          plt.title('Seasonal Component')\n",
        "          plt.legend(loc='best')\n",
        "\n",
        "          plt.subplot(414)\n",
        "          plt.plot(decomposition.resid, label='Residuals', color='red')\n",
        "          plt.title('Residual Component')\n",
        "          plt.legend(loc='best')\n",
        "\n",
        "          plt.tight_layout()\n",
        "          plt.show()\n",
        "\n",
        "      except ValueError as e:\n",
        "          print(f\"Error during additive decomposition for {book_title}: {e}\")\n",
        "\n",
        "      # Check for zero or negative values in sales\n",
        "      if (sales <= 0).any():\n",
        "          print(f\"Additive decomposition is enforced due to zero or negative values in {book_title}.\")\n",
        "      else:\n",
        "          # If residual standard deviation indicates multiplicative is better, try that\n",
        "          if decomposition.resid.std() > 0.5 * sales.std():\n",
        "              print(f\"Using multiplicative decomposition for {book_title}.\")\n",
        "\n",
        "              # Perform multiplicative decomposition\n",
        "              decomposition = seasonal_decompose(sales, model='multiplicative', period=52)\n",
        "\n",
        "              # Plot the decomposition components for multiplicative\n",
        "              plt.figure(figsize=(12, 8))\n",
        "              plt.subplot(411)\n",
        "              plt.plot(df_book.index, sales, label='Original', color='blue')\n",
        "              plt.title(f'Original Sales Data for {book_title}')\n",
        "              plt.legend(loc='best')\n",
        "\n",
        "              plt.subplot(412)\n",
        "              plt.plot(decomposition.trend, label='Trend', color='orange')\n",
        "              plt.title('Trend Component (Multiplicative)')\n",
        "              plt.legend(loc='best')\n",
        "\n",
        "              plt.subplot(413)\n",
        "              plt.plot(decomposition.seasonal, label='Seasonality', color='green')\n",
        "              plt.title('Seasonal Component (Multiplicative)')\n",
        "              plt.legend(loc='best')\n",
        "\n",
        "              plt.subplot(414)\n",
        "              plt.plot(decomposition.resid, label='Residuals', color='red')\n",
        "              plt.title('Residual Component (Multiplicative)')\n",
        "              plt.legend(loc='best')\n",
        "\n",
        "              plt.tight_layout()\n",
        "              plt.show()\n",
        "\n",
        "          else:\n",
        "              print(f\"Additive decomposition appears suitable for {book_title}.\")\n",
        "  else:\n",
        "      print(f\"Not enough data for decomposition for {book_title}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nBDiSsRW1jZ"
      },
      "source": [
        "## ACF & PACF plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcOr2uqJUoJn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "\n",
        "# ACF Plot for The Alchemist\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(2, 1, 1)\n",
        "plot_acf(alchemist['Volume'], lags=20, ax=plt.gca())\n",
        "plt.title('ACF for The Alchemist')\n",
        "\n",
        "# PACF Plot for The Alchemist\n",
        "plt.subplot(2, 1, 2)\n",
        "plot_pacf(alchemist['Volume'], lags=20, ax=plt.gca())\n",
        "plt.title('PACF for The Alchemist')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ACF Plot for The Very Hungry Caterpillar\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(2, 1, 1)\n",
        "plot_acf(caterpillar['Volume'], lags=20, ax=plt.gca())\n",
        "plt.title('ACF for The Very Hungry Caterpillar')\n",
        "\n",
        "# PACF Plot for The Very Hungry Caterpillar\n",
        "plt.subplot(2, 1, 2)\n",
        "plot_pacf(caterpillar['Volume'], lags=20, ax=plt.gca())\n",
        "plt.title('PACF for The Very Hungry Caterpillar')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22Zt3gPjUSW8"
      },
      "source": [
        "## Stationarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og7-prA_T2OX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# Function to perform ADF test and print results\n",
        "def check_stationarity(series, title):\n",
        "    adf_test = adfuller(series)\n",
        "    print(f\"Results of ADF Test for {title}:\")\n",
        "    print(f\"ADF Statistic: {adf_test[0]}\")\n",
        "    print(f\"p-value: {adf_test[1]}\")\n",
        "    print(\"Critical Values:\")\n",
        "    for key, value in adf_test[4].items():\n",
        "        print(f\"   {key}: {value}\")\n",
        "\n",
        "    # Interpretation\n",
        "    if adf_test[1] <= 0.05:\n",
        "        print(f\"{title} is stationary (reject null hypothesis)\")\n",
        "    else:\n",
        "        print(f\"{title} is not stationary (fail to reject null hypothesis)\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# For The Alchemist\n",
        "check_stationarity(alchemist['Volume'], \"The Alchemist\")\n",
        "\n",
        "# For The Very Hungry Caterpillar\n",
        "check_stationarity(caterpillar['Volume'], \"The Very Hungry Caterpillar\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dJSMFOitnnq"
      },
      "outputs": [],
      "source": [
        "# From the previous dataframe 'caterpillar' of book 'very hungry caterpillar'\n",
        "# Perform differencing on the 'Volume' column and overwrite it\n",
        "caterpillar['Volume'] = caterpillar['Volume'].diff()\n",
        "\n",
        "# Drop missing values that result from differencing\n",
        "caterpillar = caterpillar.dropna()\n",
        "\n",
        "# Display the resulting dataframe\n",
        "print(caterpillar)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGbZcyKCEeht"
      },
      "source": [
        "Now check stationarity after differencing of Very Hungry Caterpillar series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrmVvwyTt598"
      },
      "outputs": [],
      "source": [
        "# Check for stationarity with the ADF test\n",
        "adf_result = adfuller(caterpillar['Volume'])  # Perform ADF on the non-null values\n",
        "print(f'ADF Statistic: {adf_result[0]}')\n",
        "print(f'p-value: {adf_result[1]}')\n",
        "\n",
        "# If p-value < 0.05, the data is stationary\n",
        "if adf_result[1] > 0.05:\n",
        "    print(\"Series is still not stationary. Consider further transformations.\")\n",
        "else:\n",
        "    print(\"Series is stationary.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-31_vQlCLp5"
      },
      "source": [
        "## ARIMA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVW0k5cLwLzE"
      },
      "source": [
        "### The Alchemist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYXm1uhYHFsl"
      },
      "source": [
        "Performing Auto ARIMA on 'The Alchemist' book."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "450n_2q1etdK"
      },
      "outputs": [],
      "source": [
        "# Forecast horizon\n",
        "forecast_horizon = 32\n",
        "\n",
        "def auto_arima_forecast(df_book, book_title):\n",
        "    print(f\"\\nAuto ARIMA for {book_title}\")\n",
        "    sales = df_book['Volume']\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    train_data = sales[:-forecast_horizon]\n",
        "    test_data = sales[-forecast_horizon:]\n",
        "\n",
        "    # Perform Auto ARIMA model selection\n",
        "    model_auto_arima = auto_arima(\n",
        "        train_data,\n",
        "        seasonal=True,\n",
        "        m=52,                               # Weekly seasonality m = 52\n",
        "        max_p=5, max_q=5, max_P=5, max_Q=5, # Upper bounds for p, q, P, Q set to 5\n",
        "        d=0,                                # Differencing order\n",
        "        max_d=2,\n",
        "        start_p=1, start_q=1, start_P=0, start_Q=0,\n",
        "        stepwise=True,\n",
        "        trace=True,\n",
        "        error_action='ignore',\n",
        "        suppress_warnings=True,\n",
        "    )\n",
        "\n",
        "    # Get the best model parameters\n",
        "    print(f\"Best ARIMA Model: {model_auto_arima.order}\")\n",
        "\n",
        "    # Fit the best ARIMA model on the training data\n",
        "    model = ARIMA(train_data, order=model_auto_arima.order)\n",
        "    model_fit = model.fit()\n",
        "\n",
        "    # Print model summary\n",
        "    print(model_fit.summary())\n",
        "\n",
        "    # Get fitted values for the training period\n",
        "    fitted_values = model_fit.fittedvalues  # These are the in-sample predictions\n",
        "\n",
        "    # Calculate residuals for the training period (actual - fitted)\n",
        "    residuals_train = train_data - fitted_values\n",
        "\n",
        "    # Plot residuals (training period)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(train_data.index, residuals_train, label='Training Residuals', color='orange')\n",
        "    plt.title(f\"Residuals for {book_title}\")\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Residual')\n",
        "    plt.axhline(y=0, color='black', linestyle='--')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Forecast the final 32 weeks (true forecasting, future values)\n",
        "    forecast_results = model_fit.get_forecast(steps=forecast_horizon)\n",
        "\n",
        "    # Get forecasted values and confidence intervals for future periods\n",
        "    forecast = forecast_results.predicted_mean\n",
        "    conf_int = forecast_results.conf_int()\n",
        "\n",
        "    # Create a date range for the forecast (future dates)\n",
        "    forecast_index = pd.date_range(start=sales.index[-1] + pd.Timedelta(weeks=1), periods=forecast_horizon, freq='W-SAT')\n",
        "\n",
        "    # Calculate MAE and MAPE\n",
        "    mae = mean_absolute_error(test_data, forecast)\n",
        "    mape = mean_absolute_percentage_error(test_data, forecast) * 100\n",
        "\n",
        "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
        "\n",
        "    # Print actual vs forecasted values\n",
        "    actual_vs_forecast = pd.DataFrame({\n",
        "        'Actual': test_data.values,\n",
        "        'Forecasted': forecast.values,\n",
        "        'Forecast date': forecast_index\n",
        "    })\n",
        "    print(\"\\nActual vs Forecasted Values:\")\n",
        "    print(actual_vs_forecast)\n",
        "\n",
        "    # Plot the actual sales, forecast\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot the original data (actual sales)\n",
        "    plt.plot(sales, label='Actual Sales', color='blue')\n",
        "\n",
        "    # Plot forecast (out-of-sample)\n",
        "    plt.plot(forecast_index, forecast, label='Forecast', color='green')\n",
        "\n",
        "    # Plot confidence intervals\n",
        "    plt.fill_between(forecast_index,\n",
        "                     conf_int.iloc[:, 0], conf_int.iloc[:, 1],\n",
        "                     color='lightgreen', alpha=0.5, label='95% Confidence Interval')\n",
        "\n",
        "    plt.title(f\"Actual Sales and Forecast for {book_title}\")\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Sales Volume')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    return model_fit\n",
        "\n",
        "# Perform Auto ARIMA and forecasting for \"The Alchemist\"\n",
        "model_alchemist = auto_arima_forecast(alchemist,'The Alchemist')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUNroi3swSP-"
      },
      "source": [
        "### The Very Hungry Caterpillar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S_sgab56cXv"
      },
      "source": [
        "Note : The separate function was created for this book to set parameter range (p,q = 2), as the system RAM was crashing when parameters upper bound were (p ,q = 5 )increased for this book."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B67J5FjT-xIm"
      },
      "outputs": [],
      "source": [
        "# Forecast horizon\n",
        "forecast_horizon = 32\n",
        "\n",
        "def auto_arima_forecast(df_book, book_title):\n",
        "    print(f\"\\nAuto ARIMA for {book_title}\")\n",
        "    sales = df_book['Volume']\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    train_data = sales[:-forecast_horizon]\n",
        "    test_data = sales[-forecast_horizon:]\n",
        "\n",
        "    # Perform Auto ARIMA model selection\n",
        "    model_auto_arima = auto_arima(\n",
        "        train_data,\n",
        "        seasonal=True,\n",
        "        m=52,                               # Weekly seasonality m = 52\n",
        "        max_p=2, max_q=2, max_P=2, max_Q=2, # Upper bounds for p, q, P, Q is set to 2\n",
        "        d=0,                                # Differencing order\n",
        "        max_d=2,\n",
        "        start_p=1, start_q=1, start_P=0, start_Q=0,\n",
        "        stepwise=True,\n",
        "        trace=True,\n",
        "        error_action='ignore',\n",
        "        suppress_warnings=True,\n",
        "    )\n",
        "\n",
        "    # Get the best model parameters\n",
        "    print(f\"Best ARIMA Model: {model_auto_arima.order}\")\n",
        "\n",
        "    # Fit the best ARIMA model on the training data\n",
        "    model = ARIMA(train_data, order=model_auto_arima.order)\n",
        "    model_fit = model.fit()\n",
        "\n",
        "    # Print model summary\n",
        "    print(model_fit.summary())\n",
        "\n",
        "    # Get fitted values for the training period\n",
        "    fitted_values = model_fit.fittedvalues  # These are the in-sample predictions\n",
        "\n",
        "    # Calculate residuals for the training period (actual - fitted)\n",
        "    residuals_train = train_data - fitted_values\n",
        "\n",
        "    # Plot residuals (training period)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(train_data.index, residuals_train, label='Training Residuals', color='orange')\n",
        "    plt.title(f\"Residuals for {book_title}\")\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Residual')\n",
        "    plt.axhline(y=0, color='black', linestyle='--')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Forecast the final 32 weeks (true forecasting, future values)\n",
        "    forecast_results = model_fit.get_forecast(steps=forecast_horizon)\n",
        "\n",
        "    # Get forecasted values and confidence intervals for future periods\n",
        "    forecast = forecast_results.predicted_mean\n",
        "    conf_int = forecast_results.conf_int()\n",
        "\n",
        "    # Create a date range for the forecast (future dates)\n",
        "    forecast_index = pd.date_range(start=sales.index[-1] + pd.Timedelta(weeks=1), periods=forecast_horizon, freq='W-SAT')\n",
        "\n",
        "    # Calculate MAE and MAPE\n",
        "    mae = mean_absolute_error(test_data, forecast)\n",
        "    mape = mean_absolute_percentage_error(test_data, forecast) * 100\n",
        "\n",
        "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
        "\n",
        "    # Print actual vs forecasted values\n",
        "    actual_vs_forecast = pd.DataFrame({\n",
        "        'Actual': test_data.values,\n",
        "        'Forecasted': forecast.values,\n",
        "        'Forecast date': forecast_index\n",
        "    })\n",
        "    print(\"\\nActual vs Forecasted Values:\")\n",
        "    print(actual_vs_forecast)\n",
        "\n",
        "    # Plot the actual sales, forecast\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot the original data (actual sales)\n",
        "    plt.plot(sales, label='Actual Sales', color='blue')\n",
        "\n",
        "    # Plot forecast (out-of-sample)\n",
        "    plt.plot(forecast_index, forecast, label='Forecast', color='green')\n",
        "\n",
        "    # Plot confidence intervals\n",
        "    plt.fill_between(forecast_index,\n",
        "                     conf_int.iloc[:, 0], conf_int.iloc[:, 1],\n",
        "                     color='lightgreen', alpha=0.5, label='95% Confidence Interval')\n",
        "\n",
        "    plt.title(f\"Actual Sales and Forecast for {book_title}\")\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Sales Volume')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    return model_fit\n",
        "# Perform Auto ARIMA and forecasting for \"Very Hungry Caterpillar, The\"\n",
        "model_caterpillar = auto_arima_forecast(caterpillar,'Very Hungry Caterpillar, The')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML & DL models"
      ],
      "metadata": {
        "id": "3_8DPdFXiWyQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA94crEPxABR"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B57whEquA8PX"
      },
      "source": [
        "Next trying out ML model, by creating the required pipeline for the XGBoost model. Performing cross - validation and parameter tuning (including window_length) using grid search. The forecast horizon is 32 weeks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAkdardnEXiw"
      },
      "source": [
        "A function was created for both books, by setting window length and parameter value range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf_8yYigwAY3"
      },
      "outputs": [],
      "source": [
        "# Create a function to create lagged features\n",
        "def create_lagged_features(df, window_length):\n",
        "    \"\"\"Create lag features for the time series.\"\"\"\n",
        "    df_lagged = pd.DataFrame()\n",
        "    for i in range(1, window_length + 1):\n",
        "        df_lagged[f'lag_{i}'] = df['Volume'].shift(i)\n",
        "    df_lagged['Volume'] = df['Volume']\n",
        "    return df_lagged.dropna()\n",
        "\n",
        "# Custom function to calculate MAPE\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Scaling the data\n",
        "    ('xgb', XGBRegressor())  # XGBoost Regressor\n",
        "])\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'xgb__n_estimators': [100, 200],  # Number of trees\n",
        "    'xgb__learning_rate': [0.01, 0.02, 0.03, 0.1],  # Learning rate\n",
        "    'xgb__max_depth': [3, 4, 5, 6],  # Maximum depth of a tree\n",
        "    'xgb__subsample': [0.7, 0.8, 0.9, 1.0],  # Subsampling ratio\n",
        "    'xgb__colsample_bytree': [0.7, 0.8, 0.9, 1.0],  # Subsampling ratio for columns\n",
        "}\n",
        "\n",
        "# Custom scoring function combining MAE and MAPE\n",
        "scoring = {\n",
        "    'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
        "    'MAPE': make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
        "}\n",
        "\n",
        "# Function to perform grid search and forecast\n",
        "def xgboost_forecast(df, book_title, window_lengths=[1, 2, 3, 4, 5]):\n",
        "    print(f\"\\nTraining and forecasting for {book_title}\")\n",
        "\n",
        "    sales = df['Volume'].dropna()\n",
        "\n",
        "    # Split data into training and testing\n",
        "    train_data = sales[:-32]\n",
        "    test_data = sales[-32:]\n",
        "\n",
        "    best_score = float('inf')\n",
        "    best_model = None\n",
        "    best_window_length = None\n",
        "    best_y_test = None\n",
        "    best_y_pred = None\n",
        "\n",
        "    for window_length in window_lengths:\n",
        "        # Create lagged features for the training data\n",
        "        df_train_lagged = create_lagged_features(train_data.to_frame(), window_length)\n",
        "        X_train = df_train_lagged.drop('Volume', axis=1)\n",
        "        y_train = df_train_lagged['Volume']\n",
        "\n",
        "        # TimeSeriesSplit for cross-validation\n",
        "        tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "        # Grid search with cross-validation\n",
        "        grid_search = GridSearchCV(pipeline, param_grid, cv=tscv, scoring=scoring, refit='MAE', n_jobs=-1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Best model for the current window length\n",
        "        best_model_current = grid_search.best_estimator_\n",
        "        best_window_length_current = window_length\n",
        "\n",
        "        # Create lagged features for the test data\n",
        "        df_test_lagged = create_lagged_features(test_data.to_frame(), window_length)\n",
        "        X_test = df_test_lagged.drop('Volume', axis=1)\n",
        "        y_test = df_test_lagged['Volume']\n",
        "\n",
        "        # Predict on the test data\n",
        "        y_pred = best_model_current.predict(X_test)\n",
        "\n",
        "        # Calculate MAE and MAPE\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "\n",
        "        print(f\"Window Length: {window_length} | MAE: {mae:.2f} | MAPE: {mape:.2f}%\")\n",
        "\n",
        "\n",
        "        # Store the best model and predictions if current one is better\n",
        "        if mae < best_score:\n",
        "            best_score = mae\n",
        "            best_model = best_model_current\n",
        "            best_window_length = best_window_length_current\n",
        "            best_y_test = y_test\n",
        "            best_y_pred = y_pred\n",
        "\n",
        "\n",
        "    # Display expected and predicted values for the best window length\n",
        "    print(f\"\\nExpected vs Predicted values for {book_title}:\")\n",
        "    expected_vs_predicted = pd.DataFrame({\n",
        "        'Expected': best_y_test.values,\n",
        "        'Predicted': best_y_pred\n",
        "    })\n",
        "    print(expected_vs_predicted)\n",
        "\n",
        "    # Best MAE and MAPE\n",
        "    best_mape = mean_absolute_percentage_error(best_y_test, best_y_pred)\n",
        "    print(f\"\\nBest Window Length: {best_window_length} | Best MAE: {best_score:.2f} | Best MAPE: {best_mape:.2f}%\")\n",
        "\n",
        "\n",
        "    # Final forecasting on the test set using the best model\n",
        "    print(f\"\\nBest window length for {book_title}: {best_window_length}\")\n",
        "    df_final_lagged = create_lagged_features(sales.to_frame(), best_window_length)\n",
        "    X_final = df_final_lagged.drop('Volume', axis=1)\n",
        "    y_final = df_final_lagged['Volume']\n",
        "\n",
        "    forecast = best_model.predict(X_final[-32:])\n",
        "     # Generate future date indexes for the forecast\n",
        "    forecast_index = pd.date_range(start=sales.index[-1] + pd.Timedelta(weeks=1), periods=32, freq='W')\n",
        "\n",
        "    # Create a DataFrame for the forecast with future dates\n",
        "    forecast_df = pd.DataFrame({\n",
        "        'Forecast Date': forecast_index,\n",
        "        'Forecasted data': forecast\n",
        "    })\n",
        "\n",
        "    # Plot the actual sales, predictions, and forecast\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot the original data (actual sales)\n",
        "    plt.plot(sales.index, sales.values, label='Original Data', color='blue')\n",
        "\n",
        "    # Plot the forecasted values\n",
        "    forecast_index = X_final.index[-len(forecast):]\n",
        "    plt.plot(forecast_index, forecast, label='Forecasted Values', color='green')\n",
        "\n",
        "    plt.title(f\"Actual Sales and Forecast for {book_title}\")\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Sales Volume')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return best_model,forecast_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCL8vZknA4FO"
      },
      "source": [
        "### The Alchemist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kASDWFCw9RxX"
      },
      "outputs": [],
      "source": [
        "# Example usage for \"The Alchemist\"\n",
        "xgboost_forecast(df_books_of_interest[df_books_of_interest['Title'] == 'Alchemist, The'], 'The Alchemist')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7VTpfSDAsa9"
      },
      "source": [
        "### The Very Hungry Caterpillar"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running XGBoost_forecast function for this book."
      ],
      "metadata": {
        "id": "Mb4ybqCccJDR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5o56SYO5L9-"
      },
      "outputs": [],
      "source": [
        "# \"The Very Hungry Caterpillar\"\n",
        "xgboost_forecast(df_books_of_interest[df_books_of_interest['Title'] == 'Very Hungry Caterpillar, The'], 'The Very Hungry Caterpillar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGYCKLMF6L8q"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next is Creating an LSTM model for both books.\n",
        "Using KerasTuner, and performing hyperparameter tuning using the training data, for both books to predict sales data of final 32 weeks."
      ],
      "metadata": {
        "id": "M99aGyo6ceXO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSuWrza7Yy1L"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters\n",
        "n_input = 52  # Number of previous timesteps to use\n",
        "n_features = 1  # Number of features\n",
        "forecast_horizon = 32\n",
        "\n",
        "# Function to handle outliers using IQR method\n",
        "def handle_outliers(data):\n",
        "    Q1 = np.percentile(data, 25)\n",
        "    Q3 = np.percentile(data, 75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    # Replace outliers with NaN\n",
        "    data[data < lower_bound] = np.nan\n",
        "    data[data > upper_bound] = np.nan\n",
        "    # Fill NaN values with the median\n",
        "    return data.fillna(data.median())\n",
        "\n",
        "# Function to prepare data for LSTM\n",
        "def prepare_data_lstm(sales_train, n_input, n_features):\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Scale the data\n",
        "    scaled_data = scaler.fit_transform(sales_train.values.reshape(-1, 1))\n",
        "\n",
        "    # Create a TimeseriesGenerator\n",
        "    generator = TimeseriesGenerator(scaled_data, scaled_data, length=n_input, batch_size=32)\n",
        "\n",
        "    return generator, scaler\n",
        "\n",
        "# Define the LSTM model\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add LSTM layers\n",
        "    for i in range(hp.Int('num_lstm_layers', 1, 3)):\n",
        "        model.add(LSTM(units=hp.Int(f'units_lstm_{i}', min_value=32, max_value=128, step=32),\n",
        "                       activation='relu',\n",
        "                       return_sequences=(i < hp.Int('num_lstm_layers', 1, 3) - 1),\n",
        "                       input_shape=(n_input, n_features) if i == 0 else None))\n",
        "        model.add(Dropout(hp.Float(f'dropout_lstm_{i}', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "\n",
        "    # Dense output layer\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = hp.Choice('optimizer', ['adam', 'rmsprop'])\n",
        "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
        "\n",
        "    if optimizer == 'adam':\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "    else:\n",
        "        model.compile(optimizer='rmsprop', loss='mse')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to forecast using the best LSTM model\n",
        "def forecast_lstm(model, df, scaler, n_input, n_features, forecast_horizon):\n",
        "    forecast = []\n",
        "\n",
        "    # Get the last n_input data points, scaled\n",
        "    input_seq = scaler.transform(df.values[-n_input:].reshape(-1, n_features))\n",
        "\n",
        "    for _ in range(forecast_horizon):\n",
        "        input_seq = input_seq.reshape((1, n_input, n_features))  # Reshape for LSTM input\n",
        "        prediction = model.predict(input_seq, verbose=0)  # Make prediction\n",
        "        forecast.append(prediction[0, 0])  # Append prediction to forecast\n",
        "\n",
        "        # Update the input sequence for the next prediction\n",
        "        input_seq = np.append(input_seq[:, 1:, :], prediction.reshape((1, 1, n_features)), axis=1)\n",
        "\n",
        "    return scaler.inverse_transform(np.array(forecast).reshape(-1, 1))  # Reverse scaling\n",
        "\n",
        "def tune_lstm_model(generator):\n",
        "    tuner = RandomSearch(\n",
        "        build_model,\n",
        "        objective='val_loss',\n",
        "        max_trials=10,  # Increased number of trials to explore more combinations\n",
        "        executions_per_trial=1,\n",
        "        directory='lstm_tuner',\n",
        "        project_name='lstm_sales_extended')\n",
        "\n",
        "    # Run the search\n",
        "    tuner.search(generator, epochs=100, validation_data=generator, verbose=1)\n",
        "\n",
        "    # Get the best model and trial info\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]  # Return the best model\n",
        "    best_trial = tuner.oracle.get_best_trials(num_trials=1)[0]  # Get best trial\n",
        "\n",
        "    # Print trial number and parameters\n",
        "    print(f\"Best trial number: {best_trial.trial_id}\")\n",
        "    print(\"Best hyperparameters:\")\n",
        "    for param, value in best_trial.hyperparameters.values.items():\n",
        "        print(f\"{param}: {value}\")\n",
        "\n",
        "    return best_model\n",
        "\n",
        "\n",
        "# Function to train and predict sales for both books\n",
        "def lstm_forecast(df, book_title):\n",
        "    print(f\"\\nTraining and forecasting for {book_title}\")\n",
        "\n",
        "    # Handle outliers in the sales data\n",
        "    df['Volume'] = handle_outliers(df['Volume'])\n",
        "\n",
        "    sales = df['Volume']\n",
        "    sales_train = sales[:-forecast_horizon]  # Training data\n",
        "    sales_test = sales[-forecast_horizon:]\n",
        "\n",
        "    # Prepare the data for LSTM\n",
        "    generator, scaler = prepare_data_lstm(sales_train, n_input, n_features)\n",
        "\n",
        "    # Tune hyperparameters using KerasTuner\n",
        "    best_model = tune_lstm_model(generator)\n",
        "\n",
        "    # Train the model\n",
        "    history = best_model.fit(generator, epochs=100, validation_data=generator,\n",
        "                             callbacks=[EarlyStopping(patience=5, restore_best_weights=True)])\n",
        "\n",
        "    # Forecast the next 32 weeks\n",
        "    forecast = forecast_lstm(best_model, sales, scaler, n_input, n_features, forecast_horizon)\n",
        "\n",
        "    # Calculate MAE and MAPE\n",
        "    mae = mean_absolute_error(sales_test, forecast)\n",
        "    mape = mean_absolute_percentage_error(sales_test, forecast) * 100  # Convert to percentage\n",
        "\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
        "\n",
        "    # Plot the actual sales and forecast\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot original data\n",
        "    plt.plot(sales.index, sales.values, label='Actual Sales', color='blue')\n",
        "\n",
        "    # Plot forecast\n",
        "    future_index = pd.date_range(start=sales.index[-1], periods=forecast_horizon + 1, freq='W-SAT')[1:]\n",
        "    plt.plot(future_index, forecast, label='Forecasted Sales', color='green')\n",
        "\n",
        "    plt.title(f\"Sales Forecast for {book_title}\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Sales Volume\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Show expected vs forecast values\n",
        "    comparison_df = pd.DataFrame({'Actual Sales': sales_test.values, 'Forecasted Sales': forecast.flatten()})\n",
        "\n",
        "    return comparison_df, best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJlltJX-mvT5"
      },
      "source": [
        "### The Alchemist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f311flg-Y9tC"
      },
      "outputs": [],
      "source": [
        "# The Alchemist\n",
        "lstm_forecast(df_books_of_interest[df_books_of_interest['Title'] == 'Alchemist, The'], 'The Alchemist')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHZRJPDMQeiy"
      },
      "source": [
        "### The Very Hungry Caterpillar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBidH9IWLH6y"
      },
      "outputs": [],
      "source": [
        "# The Very Hungry Caterpillar\n",
        "lstm_forecast(df_books_of_interest[df_books_of_interest['Title'] == 'Very Hungry Caterpillar, The'], 'Very Hungry Caterpillar, The')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid Models"
      ],
      "metadata": {
        "id": "WGTQd2LtiDT8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo9YUrFOCmHf"
      },
      "source": [
        "## SARIMA and LSTM in sequential combination"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next creating a hybrid model of SARIMA and LSTM in sequential combination wherein the residuals from SARIMA will be forecasted using LSTM.\n",
        "The final prediction will be the sum of the predictions from SARIMA and LSTM. The LSTM will be trained on the residuals obtained during the training of the SARIMA model.\n",
        "The forecast horizon will be the final 32 weeks.\n",
        "Using KerasTuner to get the best model.\n"
      ],
      "metadata": {
        "id": "kHzCQ21Fh1wH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVRsHrg_smsO"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters\n",
        "n_input = 52  # Number of previous timesteps to use for LSTM\n",
        "n_features = 1\n",
        "forecast_horizon = 32\n",
        "\n",
        "# Function to prepare data for LSTM\n",
        "def prepare_data_lstm(residuals, n_input, n_features):\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(residuals.values.reshape(-1, 1))\n",
        "\n",
        "    generator = TimeseriesGenerator(scaled_data, scaled_data, length=n_input, batch_size=32)\n",
        "\n",
        "    return generator, scaler\n",
        "\n",
        "# Define the LSTM model\n",
        "def build_lstm_model(hp):\n",
        "    model = Sequential()\n",
        "    # Add LSTM layers\n",
        "    for i in range(hp.Int('num_lstm_layers', 1, 3)):\n",
        "        model.add(LSTM(units=hp.Int(f'units_lstm_{i}', min_value=32, max_value=128, step=32),\n",
        "                       activation='relu',\n",
        "                       return_sequences=(i < hp.Int('num_lstm_layers', 1, 3) - 1),\n",
        "                       input_shape=(n_input, n_features) if i == 0 else None))\n",
        "        model.add(Dropout(hp.Float(f'dropout_lstm_{i}', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "\n",
        "    # Dense output layer\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = hp.Choice('optimizer', ['adam', 'rmsprop'])\n",
        "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
        "\n",
        "    if optimizer == 'adam':\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "    else:\n",
        "        model.compile(optimizer='rmsprop', loss='mse')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to run hyperparameter tuning using KerasTuner\n",
        "def tune_lstm_model(generator):\n",
        "    tuner = RandomSearch(\n",
        "        build_lstm_model,\n",
        "        objective='val_loss',\n",
        "        max_trials=10,\n",
        "        executions_per_trial=1,\n",
        "        directory='lstm_tuner',\n",
        "        project_name='lstm_sales')\n",
        "\n",
        "    tuner.search(generator, epochs=50, validation_data=generator)\n",
        "\n",
        "    # Get the best model and trial info\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]  # Return the best model\n",
        "    best_trial = tuner.oracle.get_best_trials(num_trials=1)[0]  # Get best trial\n",
        "\n",
        "    # Print best model and trial information\n",
        "    print(\"Best model summary:\")\n",
        "    best_model.summary()\n",
        "\n",
        "    print(f\"\\nBest trial number: {best_trial.trial_id}\")\n",
        "    print(\"Best hyperparameters:\")\n",
        "    for param, value in best_trial.hyperparameters.values.items():\n",
        "        print(f\"{param}: {value}\")\n",
        "\n",
        "    return best_model\n",
        "\n",
        "# Function to handle outliers using the IQR method\n",
        "def handle_outliers(df):\n",
        "    Q1 = df['Volume'].quantile(0.25)\n",
        "    Q3 = df['Volume'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Identify outliers\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Replace outliers with the median\n",
        "    df['Volume'] = np.where(\n",
        "        (df['Volume'] < lower_bound) | (df['Volume'] > upper_bound),\n",
        "        df['Volume'].median(),\n",
        "        df['Volume']\n",
        "    )\n",
        "    return df\n",
        "\n",
        "# Function to forecast using the LSTM model\n",
        "def forecast_lstm(model, residuals, scaler, n_input, n_features, forecast_horizon):\n",
        "    forecast = []\n",
        "    input_seq = scaler.transform(residuals.values[-n_input:].reshape(-1, 1))  # Get last n_input data points\n",
        "\n",
        "    for _ in range(forecast_horizon):\n",
        "        input_seq = input_seq.reshape((1, n_input, n_features))  # Reshape for LSTM input\n",
        "        prediction = model.predict(input_seq, verbose=0)  # Make prediction\n",
        "        forecast.append(prediction[0, 0])  # Append prediction to forecast\n",
        "\n",
        "        # Update the input sequence for the next prediction\n",
        "        input_seq = np.append(input_seq[:, 1:, :], prediction.reshape((1, 1, n_features)), axis=1)  # Append predicted value\n",
        "\n",
        "    return scaler.inverse_transform(np.array(forecast).reshape(-1, 1))  # Reverse scaling\n",
        "\n",
        "# Function to hybrid forecasting with SARIMA and LSTM\n",
        "def hybrid_forecast(df, book_title):\n",
        "    print(f\"\\nTraining and forecasting for {book_title}\")\n",
        "\n",
        "    # Handle outliers in the sales data\n",
        "    df = handle_outliers(df)\n",
        "\n",
        "    sales = df['Volume']\n",
        "    sales_train = sales[:-forecast_horizon]  # Training data\n",
        "    sales_test = sales[-forecast_horizon:]   # Test data\n",
        "\n",
        "    # Fit SARIMA model\n",
        "    sarima_model = sm.tsa.statespace.SARIMAX(sales_train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 52))\n",
        "    sarima_fit = sarima_model.fit(disp=False)\n",
        "\n",
        "    # Get SARIMA predictions and residuals\n",
        "    sarima_pred = sarima_fit.predict(start=len(sales_train), end=len(sales_train) + forecast_horizon - 1)\n",
        "    residuals = sales_train - sarima_fit.fittedvalues\n",
        "\n",
        "    # Prepare residuals for LSTM\n",
        "    generator, scaler = prepare_data_lstm(residuals, n_input, n_features)\n",
        "\n",
        "    # Tune hyperparameters for LSTM\n",
        "    best_lstm_model = tune_lstm_model(generator)\n",
        "\n",
        "    # Train LSTM model\n",
        "    best_lstm_model.fit(generator, epochs=100, validation_data=generator)\n",
        "\n",
        "    # Forecast the next 32 weeks with LSTM\n",
        "    lstm_forecast_residuals = forecast_lstm(best_lstm_model, residuals, scaler, n_input, n_features, forecast_horizon)\n",
        "\n",
        "    # Final forecast combining SARIMA and LSTM\n",
        "    final_forecast = sarima_pred.values + lstm_forecast_residuals.flatten()\n",
        "\n",
        "    # Calculate MAE and MAPE\n",
        "    mae = mean_absolute_error(sales_test, final_forecast)\n",
        "    mape = mean_absolute_percentage_error(sales_test, final_forecast) * 100  # Convert to percentage\n",
        "\n",
        "    print(f\"\\nMean Absolute Error (MAE): {mae:.2f}\")\n",
        "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
        "\n",
        "    # Plot the actual sales and final forecast\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot original data\n",
        "    plt.plot(sales.index, sales.values, label='Actual Sales', color='blue')\n",
        "\n",
        "    # Plot final forecast\n",
        "    future_index = pd.date_range(start=sales.index[-1], periods=forecast_horizon + 1, freq='W-SAT')[1:]\n",
        "    plt.plot(future_index, final_forecast, label='Hybrid Forecast', color='green')\n",
        "\n",
        "    plt.title(f\"Sales Forecast for {book_title} using Hybrid SARIMA-LSTM Model\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Sales Volume\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Show expected vs predicted values\n",
        "    comparison_df = pd.DataFrame({'Actual Sales': sales_test.values, 'Forecasted Sales': final_forecast})\n",
        "    print(comparison_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_FoQ3nWy_Yz"
      },
      "source": [
        "### The Alchemist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkjUhhw8x2VK"
      },
      "outputs": [],
      "source": [
        "# \"The Alchemist\"\n",
        "hybrid_forecast(df_books_of_interest[df_books_of_interest['Title'] == 'Alchemist, The'], 'The Alchemist')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqVvQvsYzDA2"
      },
      "source": [
        "### The Very Hungry Caterpillar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiEQLq-KxLcP"
      },
      "outputs": [],
      "source": [
        "# \"The Very Hungry Caterpillar\"\n",
        "hybrid_forecast(df_books_of_interest[df_books_of_interest['Title'] == 'Very Hungry Caterpillar, The'], 'Very Hungry Caterpillar, The')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTAg2wrODtWc"
      },
      "source": [
        "## SARIMA and LSTM in parallel combination"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next creating a hybrid model of SARIMA and LSTM in parallel combination wherein the residuals from SARIMA will be forecasted using LSTM.\n",
        "The final prediction will be the sum of the predictions from SARIMA and LSTM. The LSTM will be trained on the residuals obtained during the training of the SARIMA model.\n",
        "The forecast horizon will be the final 32 weeks.\n",
        "Using KerasTuner to get the best model."
      ],
      "metadata": {
        "id": "0krVhn7ooxrs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lINlYsPbDC2N"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters\n",
        "n_input = 52  # Number of previous timesteps to use for LSTM\n",
        "n_features = 1\n",
        "forecast_horizon = 32\n",
        "\n",
        "# Function to prepare data for LSTM\n",
        "def prepare_data_lstm(sales, n_input, n_features):\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(sales.values.reshape(-1, 1))\n",
        "\n",
        "    generator = TimeseriesGenerator(scaled_data, scaled_data, length=n_input, batch_size=32)\n",
        "\n",
        "    return generator, scaler\n",
        "\n",
        "# Define the LSTM model\n",
        "def build_lstm_model(hp):\n",
        "    model = Sequential()\n",
        "    # Add LSTM layers\n",
        "    for i in range(hp.Int('num_lstm_layers', 1, 3)):\n",
        "        model.add(LSTM(units=hp.Int(f'units_lstm_{i}', min_value=32, max_value=128, step=32),\n",
        "                       activation='relu',\n",
        "                       return_sequences=(i < hp.Int('num_lstm_layers', 1, 3) - 1),\n",
        "                       input_shape=(n_input, n_features) if i == 0 else None))\n",
        "        model.add(Dropout(hp.Float(f'dropout_lstm_{i}', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "\n",
        "    # Dense output layer\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = hp.Choice('optimizer', ['adam', 'rmsprop'])\n",
        "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
        "\n",
        "    if optimizer == 'adam':\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "    else:\n",
        "        model.compile(optimizer='rmsprop', loss='mse')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Function to run hyperparameter tuning using KerasTuner\n",
        "def tune_lstm_model(generator):\n",
        "    tuner = RandomSearch(\n",
        "        build_lstm_model,\n",
        "        objective='val_loss',\n",
        "        max_trials=10,\n",
        "        executions_per_trial=1,\n",
        "        directory='lstm_tuner',\n",
        "        project_name='lstm_sales')\n",
        "\n",
        "    tuner.search(generator, epochs=50, validation_data=generator)\n",
        "\n",
        "    # Get the best model and trial info\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]  # Return the best model\n",
        "    best_trial = tuner.oracle.get_best_trials(num_trials=1)[0]  # Get best trial\n",
        "\n",
        "    # Print best model and trial information\n",
        "    print(\"Best model summary:\")\n",
        "    best_model.summary()\n",
        "\n",
        "    print(f\"\\nBest trial number: {best_trial.trial_id}\")\n",
        "    print(\"Best hyperparameters:\")\n",
        "    for param, value in best_trial.hyperparameters.values.items():\n",
        "        print(f\"{param}: {value}\")\n",
        "\n",
        "    return best_model\n",
        "\n",
        "# Function to forecast using the LSTM model\n",
        "def forecast_lstm(model, sales, scaler, n_input, n_features, forecast_horizon):\n",
        "    forecast = []\n",
        "    input_seq = scaler.transform(sales.values[-n_input:].reshape(-1, 1))  # Get last n_input data points\n",
        "\n",
        "    for _ in range(forecast_horizon):\n",
        "        input_seq = input_seq.reshape((1, n_input, n_features))  # Reshape for LSTM input\n",
        "        prediction = model.predict(input_seq, verbose=0)  # Make prediction\n",
        "        forecast.append(prediction[0, 0])  # Append prediction to forecast\n",
        "\n",
        "        # Update the input sequence for the next prediction\n",
        "        input_seq = np.append(input_seq[:, 1:, :], prediction.reshape((1, 1, n_features)), axis=1)  # Append predicted value\n",
        "\n",
        "    return scaler.inverse_transform(np.array(forecast).reshape(-1, 1))  # Reverse scaling\n",
        "# Function to handle outliers using the IQR method\n",
        "def handle_outliers(df):\n",
        "    Q1 = df['Volume'].quantile(0.25)\n",
        "    Q3 = df['Volume'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Identify outliers\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Replace outliers with the median\n",
        "    df['Volume'] = np.where(\n",
        "        (df['Volume'] < lower_bound) | (df['Volume'] > upper_bound),\n",
        "        df['Volume'].median(),\n",
        "        df['Volume']\n",
        "    )\n",
        "    return df\n",
        "\n",
        "# Function to parallel forecasting with SARIMA and LSTM\n",
        "def parallel_hybrid_forecast(df, book_title, sarima_weight=0.5, lstm_weight=0.5):\n",
        "    print(f\"\\nTraining and forecasting for {book_title}\")\n",
        "\n",
        "    # Handle outliers in the sales data\n",
        "    df = handle_outliers(df)\n",
        "\n",
        "    sales = df['Volume']\n",
        "    sales_train = sales[:-forecast_horizon]  # Training data\n",
        "    sales_test = sales[-forecast_horizon:]   # Test data\n",
        "\n",
        "    # Fit SARIMA model\n",
        "    sarima_model = sm.tsa.statespace.SARIMAX(sales_train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 52))\n",
        "    sarima_fit = sarima_model.fit(disp=False)\n",
        "\n",
        "    # Get SARIMA predictions for the forecast horizon\n",
        "    sarima_pred = sarima_fit.predict(start=len(sales_train), end=len(sales_train) + forecast_horizon - 1)\n",
        "\n",
        "    # Prepare data for LSTM\n",
        "    generator, scaler = prepare_data_lstm(sales_train, n_input, n_features)\n",
        "\n",
        "    # Tune hyperparameters for LSTM\n",
        "    best_lstm_model = tune_lstm_model(generator)\n",
        "\n",
        "    # Train LSTM model\n",
        "    best_lstm_model.fit(generator, epochs=50, validation_data=generator)\n",
        "\n",
        "    # Forecast the next 32 weeks with LSTM\n",
        "    lstm_forecast = forecast_lstm(best_lstm_model, sales_train, scaler, n_input, n_features, forecast_horizon)\n",
        "\n",
        "\n",
        "    final_forecast = sarima_weight * sarima_pred.values + lstm_weight * lstm_forecast.flatten() # Combine predictions using weighted average\n",
        "\n",
        "    # Calculate MAE and MAPE\n",
        "    mae = mean_absolute_error(sales_test, final_forecast)\n",
        "    mape = mean_absolute_percentage_error(sales_test, final_forecast) * 100  # Convert to percentage\n",
        "\n",
        "    print(f\"\\nMean Absolute Error (MAE): {mae}\")\n",
        "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
        "\n",
        "    # Plot the actual sales and final forecast\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot original data\n",
        "    plt.plot(sales.index, sales.values, label='Actual Sales', color='blue')\n",
        "\n",
        "    # Plot final forecast\n",
        "    future_index = pd.date_range(start=sales.index[-1], periods=forecast_horizon + 1, freq='W-SAT')[1:]\n",
        "    plt.plot(future_index, final_forecast, label='Hybrid Forecast (SARIMA + LSTM)', color='green')\n",
        "\n",
        "    plt.title(f\"Sales Forecast for {book_title} using Parallel Hybrid SARIMA-LSTM Model\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Sales Volume\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Show expected vs predicted values\n",
        "    comparison_df = pd.DataFrame({'Actual Sales': sales_test.values, 'Forecasted Sales': final_forecast})\n",
        "    print(comparison_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jsFm31dA0eB"
      },
      "source": [
        "### The Alchemist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zIpuGEl-kR4"
      },
      "outputs": [],
      "source": [
        "# The Alchemist - Applying weights (sarima_weight=0.5, lstm_weight=0.5)\n",
        "parallel_hybrid_forecast(df_books_of_interest[df_books_of_interest['Title'] == 'Alchemist, The'], 'The Alchemist', sarima_weight=0.5, lstm_weight=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The Alchemist - Applying weights (sarima_weight=0.4, lstm_weight=0.6)\n",
        "parallel_hybrid_forecast(df_books_of_interest[df_books_of_interest['Title'] == 'Alchemist, The'], 'The Alchemist', sarima_weight=0.4, lstm_weight=0.6)"
      ],
      "metadata": {
        "id": "2aQJwIGjB-ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Alchemist - Applying weights (sarima_weight=0.3, lstm_weight=0.7)\n",
        "parallel_hybrid_forecast(df_books_of_interest[df_books_of_interest['Title'] == 'Alchemist, The'], 'The Alchemist', sarima_weight=0.3, lstm_weight=0.7)"
      ],
      "metadata": {
        "id": "jOvSfK25EmXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTp_EzE7A7lf"
      },
      "source": [
        "### The Very Hungry Caterpillar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BbAvyMzDM_P"
      },
      "outputs": [],
      "source": [
        "# The Very Hungry Caterpillar - Applying weights (sarima_weight=0.5, lstm_weight=0.5)\n",
        "parallel_hybrid_forecast(df_books_of_interest[df_books_of_interest['Title'] == 'Very Hungry Caterpillar, The'], 'The Very Hungry Caterpillar', sarima_weight=0.5, lstm_weight=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The Very Hungry Caterpillar - Applying weights (sarima_weight=0.7, lstm_weight=0.3)\n",
        "parallel_hybrid_forecast(df_books_of_interest[df_books_of_interest['Title'] == 'Very Hungry Caterpillar, The'], 'The Very Hungry Caterpillar', sarima_weight=0.7, lstm_weight=0.3)"
      ],
      "metadata": {
        "id": "gZTOw4OEIIA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Very Hungry Caterpillar - Applying weights (sarima_weight=0.8, lstm_weight=0.2)\n",
        "parallel_hybrid_forecast(df_books_of_interest[df_books_of_interest['Title'] == 'Very Hungry Caterpillar, The'], 'The Very Hungry Caterpillar', sarima_weight=0.8, lstm_weight=0.2)"
      ],
      "metadata": {
        "id": "9HdCS0zmI5tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9e4-fMiHhcF"
      },
      "source": [
        "#Monthly Resampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9nYeX_aB8qH"
      },
      "source": [
        "Next, resampling was performed to change the frequency into monthly and perform analysis. Since the sales data column required for the analysis is 'Volume', the aggregation operation (mean) was performed to 'Volume' and filled in the missing values with forward fill (ffill)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqrogcryEXQO"
      },
      "outputs": [],
      "source": [
        "# Resample to monthly frequency, mean of the 'Volume' column\n",
        "monthly_df = (\n",
        "    df_books_of_interest\n",
        "    .groupby(['ISBN', 'Title'])  # Group by ISBN and Title\n",
        "    .resample('ME')           # Resample to monthly frequency\n",
        "    .agg({\n",
        "        'Volume': 'mean'           # mean of the 'Volume' column\n",
        "    })\n",
        "    .ffill()\n",
        "    .reset_index()                # Reset index to keep 'ISBN' and 'Title' as columns\n",
        ")\n",
        "\n",
        "# Display the final DataFrame\n",
        "monthly_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFlabunSLQCU"
      },
      "outputs": [],
      "source": [
        "# setting 'End Date' as index\n",
        "monthly_df.set_index('End Date', inplace=True)\n",
        "monthly_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2XMlZ9WKEXj"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next trying out ML model, by creating the required pipeline for the XGBoost model. Performing cross - validation and parameter tuning (including window_length) using grid search. The forecast horizon is 8 months."
      ],
      "metadata": {
        "id": "Mk9HpcU6pDy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to create lagged features\n",
        "def create_lagged_features(df, window_length):\n",
        "    \"\"\"Create lag and rolling mean features for the time series, handling NaN values.\"\"\"\n",
        "    df_lagged = pd.DataFrame()\n",
        "\n",
        "    # Create lag features\n",
        "    for i in range(1, window_length + 1):\n",
        "        df_lagged[f'lag_{i}'] = df['Volume'].shift(i)\n",
        "\n",
        "    # Create rolling mean features\n",
        "    df_lagged['rolling_mean_3'] = df['Volume'].rolling(window=3).mean().shift(1)\n",
        "    df_lagged['rolling_mean_6'] = df['Volume'].rolling(window=6).mean().shift(1)\n",
        "    df_lagged['rolling_mean_12'] = df['Volume'].rolling(window=12).mean().shift(1)\n",
        "\n",
        "    df_lagged['Volume'] = df['Volume']\n",
        "\n",
        "    # Fill NaN values with the mean of the column\n",
        "    df_lagged.fillna(df_lagged.mean(), inplace=True)\n",
        "\n",
        "    return df_lagged\n",
        "\n",
        "# Updated parameter grid to include regularization and tree parameters\n",
        "param_grid = {\n",
        "    'xgb__n_estimators': [100, 200],  # Number of trees\n",
        "    'xgb__learning_rate': [0.01, 0.02, 0.03, 0.04, 0.1],  # Learning rate\n",
        "    'xgb__max_depth': [3, 4, 5, 6],  # Maximum depth of a tree\n",
        "    'xgb__subsample': [0.7, 0.8, 0.9, 1.0],  # Subsampling ratio\n",
        "    'xgb__colsample_bytree': [0.7, 0.8, 0.9, 1.0],  # Subsampling ratio for columns\n",
        "}\n",
        "\n",
        "# Updated function to include outlier handling and more lags\n",
        "def xgboost_forecast(df, book_title, window_lengths=[1, 3, 6, 7, 12]):\n",
        "    print(f\"\\nTraining and forecasting for {book_title}\")\n",
        "\n",
        "    sales = df['Volume'].dropna()\n",
        "\n",
        "    # Split data into training and testing\n",
        "    train_data = sales[:-8]  # Keep the last 8 months as the test set\n",
        "    test_data = sales[-8:]   # Test set is the last 8 months\n",
        "\n",
        "    # Handle outliers by capping them\n",
        "    Q1 = train_data.quantile(0.25)\n",
        "    Q3 = train_data.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    train_data = train_data.clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "    best_score = float('inf')\n",
        "    best_model = None\n",
        "    best_window_length = None\n",
        "    best_y_test = None\n",
        "    best_y_pred = None\n",
        "\n",
        "    for window_length in window_lengths:\n",
        "        # Create lagged features for the training data\n",
        "        df_train_lagged = create_lagged_features(train_data.to_frame(), window_length)\n",
        "        X_train = df_train_lagged.drop('Volume', axis=1)\n",
        "        y_train = df_train_lagged['Volume']\n",
        "\n",
        "        # TimeSeriesSplit for cross-validation\n",
        "        tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "        # Grid search with cross-validation\n",
        "        grid_search = GridSearchCV(pipeline, param_grid, cv=tscv, scoring=scoring, refit='MAE', n_jobs=-1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Best model for the current window length\n",
        "        best_model_current = grid_search.best_estimator_\n",
        "        best_window_length_current = window_length\n",
        "\n",
        "        # Create lagged features for the test data\n",
        "        df_test_lagged = create_lagged_features(test_data.to_frame(), window_length)\n",
        "        X_test = df_test_lagged.drop('Volume', axis=1)\n",
        "        y_test = df_test_lagged['Volume']\n",
        "\n",
        "        # Predict on the test data\n",
        "        y_pred = best_model_current.predict(X_test)\n",
        "\n",
        "        # Calculate MAE and MAPE\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "\n",
        "        print(f\"Window Length: {window_length} | MAE: {mae:.2f} | MAPE: {mape:.2f}%\")\n",
        "\n",
        "        # Store the best model and predictions if current one is better\n",
        "        if mae < best_score:\n",
        "            best_score = mae\n",
        "            best_model = best_model_current\n",
        "            best_window_length = best_window_length_current\n",
        "            best_y_test = y_test\n",
        "            best_y_pred = y_pred\n",
        "\n",
        "    # Display expected and predicted values for the best window length\n",
        "    print(f\"\\nExpected vs Predicted values for {book_title}:\")\n",
        "    expected_vs_predicted = pd.DataFrame({\n",
        "        'Expected': best_y_test.values,\n",
        "        'Predicted': best_y_pred\n",
        "    })\n",
        "    print(expected_vs_predicted)\n",
        "\n",
        "    # Best MAE and MAPE\n",
        "    best_mape = mean_absolute_percentage_error(best_y_test, best_y_pred)  # Calculate MAPE for best model\n",
        "    print(f\"\\nBest Window Length: {best_window_length} | Best MAE: {best_score:.2f} | Best MAPE: {best_mape:.2f}%\")\n",
        "\n",
        "    # Final forecasting on the test set using the best model\n",
        "    print(f\"\\nBest window length for {book_title}: {best_window_length}\")\n",
        "    df_final_lagged = create_lagged_features(sales.to_frame(), best_window_length)\n",
        "    X_final = df_final_lagged.drop('Volume', axis=1)\n",
        "    y_final = df_final_lagged['Volume']\n",
        "\n",
        "    forecast = best_model.predict(X_final[-8:])  # Predict the next 8 months\n",
        "\n",
        "    # Generate future date indexes for the forecast\n",
        "    forecast_index = pd.date_range(start=sales.index[-1] + pd.DateOffset(months=1), periods=8, freq='M')\n",
        "\n",
        "    # Create a DataFrame for the forecast with future dates\n",
        "    forecast_df = pd.DataFrame({\n",
        "        'Forecast Date': forecast_index,\n",
        "        'Forecasted data': forecast\n",
        "    })\n",
        "\n",
        "    # Plot the actual sales, predictions, and forecast\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot the original data (actual sales)\n",
        "    plt.plot(sales.index, sales.values, label='Original Data', color='blue')\n",
        "\n",
        "    # Plot the forecasted values\n",
        "    plt.plot(forecast_index, forecast, label='Forecasted Values', color='green')\n",
        "\n",
        "    plt.title(f\"Actual Sales and Forecast for {book_title}\")\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Sales Volume')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return forecast_df, best_model\n"
      ],
      "metadata": {
        "id": "WOMR1-6qhTDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcygPoSYOhAI"
      },
      "source": [
        "### The Alchemist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWds72ruBpIb"
      },
      "outputs": [],
      "source": [
        "# The Alchemist\n",
        "xgboost_forecast(monthly_df[monthly_df['Title'] == 'Alchemist, The'], 'The Alchemist')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w6Wzi6OOpTm"
      },
      "source": [
        "### The Very Hungry Caterpillar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeyOy42eC1kb"
      },
      "outputs": [],
      "source": [
        "# The Very Hungry Caterpillar\n",
        "xgboost_forecast(monthly_df[monthly_df['Title'] == 'Very Hungry Caterpillar, The'], 'The Very Hungry Caterpillar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCpdMCX_MjgX"
      },
      "source": [
        "## SARIMA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next is training the SARIMA model (using Auto ARIMA) on this data. The forecast horizon is 8 months."
      ],
      "metadata": {
        "id": "wPvcKKtvtzEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sarima_forecast_monthly(df, book_title):\n",
        "    print(f\"\\nTraining and forecasting for {book_title}\")\n",
        "\n",
        "    sales = df['Volume']\n",
        "    forecast_horizon = 8\n",
        "    train_data = sales[:-forecast_horizon]\n",
        "    test_data = sales[-forecast_horizon:]\n",
        "\n",
        "    # Fit the Auto ARIMA model\n",
        "    model = auto_arima(train_data, seasonal=True, m=12,\n",
        "                       trace=True, error_action='ignore',\n",
        "                       suppress_warnings=True)\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    # Make forecast\n",
        "    forecast, conf_int = model.predict(n_periods=forecast_horizon, return_conf_int=True)\n",
        "\n",
        "    # Calculate MAE and MAPE\n",
        "    mae = mean_absolute_error(test_data, forecast)\n",
        "    mape = mean_absolute_percentage_error(test_data, forecast) * 100  # Convert to percentage\n",
        "\n",
        "    print(f\"\\nMean Absolute Error (MAE): {mae}\")\n",
        "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
        "\n",
        "    # Plot the actual sales and forecast\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(sales.index, sales.values, label='Actual Sales', color='blue')\n",
        "    forecast_index = pd.date_range(start=sales.index[-forecast_horizon], periods=forecast_horizon, freq='M')\n",
        "    plt.plot(forecast_index, forecast, label='Forecasted Sales', color='green')\n",
        "    plt.fill_between(forecast_index,\n",
        "                     conf_int[:, 0], conf_int[:, 1], color='green', alpha=0.3)\n",
        "    plt.title(f\"Sales Forecast for {book_title}\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Sales Volume\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Show expected vs predicted values\n",
        "    comparison_df = pd.DataFrame({'Actual Sales': test_data.values, 'Predicted Sales': forecast.values})\n",
        "    print(comparison_df)"
      ],
      "metadata": {
        "id": "HoGx4tu_HOIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Alchemist"
      ],
      "metadata": {
        "id": "iuwm7FVjHRAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njdEgYtDMqtx"
      },
      "outputs": [],
      "source": [
        "# The Alchemist\n",
        "sarima_forecast_monthly(monthly_df[monthly_df['Title'] == 'Alchemist, The'], 'The Alchemist')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Very Hungry Caterpillar"
      ],
      "metadata": {
        "id": "fQYKOQnXS3QO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The Very Hungry Caterpillar\n",
        "sarima_forecast_monthly(monthly_df[monthly_df['Title'] == 'Very Hungry Caterpillar, The'], 'The Very Hungry Caterpillar')"
      ],
      "metadata": {
        "id": "ViS8QHTcHFZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> The weekly and monthly frequency data both gave out better results when used with suitable models. When analysing sales data in weekly frequency for both \"The Alchemist\" and \"The Very Hungry Caterpillar,\" the XGBoost model demonstrated superior performance. It effectively captured the underlying trends and seasonality of the sales patterns, providing accurate forecasts that aligned closely with the actual data.\n",
        "In contrast, when the sales data was transformed into a monthly frequency, the SARIMA model outperformed other models. It excelled at capturing the long-term trends and seasonal fluctuations in the sales data, making it the best choice for forecasting in this scenario.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cY3dEnUCQYMP"
      }
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}